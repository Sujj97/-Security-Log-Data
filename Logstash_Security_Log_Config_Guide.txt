1. Data Ingestion
I'm using the File Input Plugin to ingest logs. In the configuration, we specify the file path to ensure Logstash reads logs from the correct source. The start_position => "beginning" ensures that Logstash processes logs from the start, even if they've been read before. Additionally, I have added sincedb_path => "NUL" to prevent Logstash from keeping track of previously read positions, which avoids missing or reprocessing logs.

file {
    path => "C:/Users/sbsuj/OneDrive/Desktop/logstash-8.17.4/bin/Security_Log.txt"
    start_position => "beginning"
    sincedb_path => "NUL" 
}

2. Filtering & Data Transformation
There are multiple ways to extract data from logs, such as using the KV Plugin, but since the entire log isn't in a key-value format, I went with Grok, which is more flexible and easier to debug.
Grok Filter Plugin is used to parse raw log messages and extract structured fields like description, hostname, source_ip, and severity.
Mutate Plugin is used to transform severity from 1 → "High". Since gsub doesn’t work on integers, we first convert severity to a string before applying gsub.
Mutate Plugin is used again to remove unnecessary fields to keep the output structured and optimized.

While there are other ways to do this (like translate or if conditions), I chose gsub which works efficiently in this case.

3. Data Output
Logstash sends the processed logs to two destinations:

a.Console Output (stdout) – Which allows me to verify and pased logs.

b.Elasticsearch (output Plugin) –The structured logs are indexed in Elasticsearch under the security index for further analysis and visualization in Kibana. 

This setup ensures that logs are efficiently processed, structured, and available for analysis.
